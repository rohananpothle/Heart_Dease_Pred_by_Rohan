# Recurrent Neural Network (RNN) ‚Äì Example Explanation

## üß† Problem: Next Word Prediction

Given the sentence:

"I am feeling very ____"

We want to predict the next word using a Recurrent Neural Network (RNN).

This is called a **Language Modeling** or **Sequence Modeling** problem.

---

## üìå Step 1: Input Representation (Word Embedding)

Each word is converted into a numerical vector using an **Embedding Layer**.

Example:

- x‚ÇÅ = "I"
- x‚ÇÇ = "am"
- x‚ÇÉ = "feeling"
- x‚ÇÑ = "very"

Each word becomes a dense vector:

x‚Çú ‚Üí Word Embedding Vector

These embeddings capture semantic meaning.

---

## üîÑ Step 2: Hidden State Computation (Memory Update)

At each time step, the hidden state is updated using:

h‚Çú = tanh(W‚Çìx‚Çú + W‚Çïh‚Çú‚Çã‚ÇÅ + b)

Where:

- x‚Çú = current word embedding
- h‚Çú‚Çã‚ÇÅ = previous hidden state
- h‚Çú = current hidden state
- W‚Çì, W‚Çï = weight matrices
- b = bias
- tanh = activation function

The hidden state acts as a **memory** of previous words.

---

## ‚è≥ Time Step Breakdown

### Time Step 1:
Input: "I"

h‚ÇÅ = f(W‚Çìx‚ÇÅ + W‚Çïh‚ÇÄ)

Memory contains: "I"

---

### Time Step 2:
Input: "am"

h‚ÇÇ = f(W‚Çìx‚ÇÇ + W‚Çïh‚ÇÅ)

Memory contains: "I am"

---

### Time Step 3:
Input: "feeling"

h‚ÇÉ = f(W‚Çìx‚ÇÉ + W‚Çïh‚ÇÇ)

Memory contains: "I am feeling"

---

### Time Step 4:
Input: "very"

h‚ÇÑ = f(W‚Çìx‚ÇÑ + W‚Çïh‚ÇÉ)

Memory contains: "I am feeling very"

This final hidden state is called the **Context Vector Representation**.

---

## üéØ Step 3: Output Prediction (Softmax Layer)

To predict the next word:

y‚ÇÖ = Softmax(Wyh‚ÇÑ)

Softmax converts output into probability distribution over vocabulary.

Example Output:

| Word   | Probability |
|--------|------------|
| happy  | 0.62       |
| sad    | 0.21       |
| tired  | 0.10       |
| angry  | 0.07       |

Model selects highest probability ‚Üí **"happy"**

---

## üìä Mathematical Objective

The model learns:

P(x‚ÇÖ | x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ)

Which means:

Probability of the next word given previous words.

---

## üî• Interview Summary

- RNN processes sequential data step-by-step.
- Hidden state stores contextual information.
- Final hidden state is used to predict next word.
- Softmax outputs probability distribution.
- Used in NLP, Time Series, and Speech tasks.
